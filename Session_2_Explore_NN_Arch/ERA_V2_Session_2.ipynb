{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "80f48a9f-c53b-47ed-97d4-0303d47ab5fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "1578f6ea-5d11-4d0c-e3c9-baf2c8920b8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size to 128\n",
        "batch_size = 128\n",
        "# Using Dataloader load the training data, convert to Tensor and normalize the data. Enable data shuffling during training.\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "# Using Dataloader load the test data, convert to Tensor and normalize the data. Enable data shuffling during testing.\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "EQZaZRGcNLtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bed5561-3afd-40d0-8630-a97048d10cd5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16345577.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 507053.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4345626.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5226482.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far.\n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\"."
      ],
      "metadata": {
        "id": "r3gEjf-xMb-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstDNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FirstDNN, self).__init__()\n",
        "    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "    # r_in:3 , n_in:28 , j_in:1 , s:1 , r_out:5 , n_out:28 , j_out:1\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    # r_in:5 , n_in:28 , j_in:1 , s:2 , r_out:6 , n_out:14 , j_out:2\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    # r_in:6 , n_in:14 , j_in:2 , s:1 , r_out:10 , n_out:14 , j_out:2\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    # r_in:10 , n_in:14 , j_in:2 , s:1 , r_out:14 , n_out:14 , j_out:2\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\n",
        "    # r_in:14 , n_in:14 , j_in:2 , s:2 , r_out:16 , n_out:7 , j_out:4\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    # r_in:16 , n_in:7 , j_in:4 , s:1 , r_out:24 , n_out:5 , j_out:4\n",
        "    self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "    # r_in:24 , n_in:5 , j_in:4 , s:1 , r_out:32 , n_out:3 , j_out:4\n",
        "    self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "    # r_in:32 , n_in:3 , j_in:4 , s:1 , r_out:40 , n_out:1 , j_out:4\n",
        "    self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "# Correct values\n",
        "# https://user-images.githubusercontent.com/498461/238034116-7db4cec0-7738-42df-8b67-afa971428d39.png\n",
        "  def forward(self, x):\n",
        "    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "    x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "    x = self.conv7(x)\n",
        "    #x = F.relu(x) # this is the last step. Think what ReLU does to our results at this stage!\n",
        "    x = x.view(-1, 10)\n",
        "    return F.log_softmax(x)\n"
      ],
      "metadata": {
        "id": "Sir2LmSVLr_4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FirstDNN().to(device)"
      ],
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__MtFIYNwXa",
        "outputId": "bfe3d950-cb5c-4453-a178-a5bf3e6b9260"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-41ccc38eb45d>:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TQDM python library to show progress bar during loop execution\n",
        "from tqdm import tqdm\n",
        "# Define the train model routine\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "# Call model training that will do read the input data and set the model in training mode (no forward prop)\n",
        "    model.train()\n",
        "# Using tqdm obtain the number of batches from train_loader\n",
        "    pbar = tqdm(train_loader)\n",
        "#Go through each batch and and do training. Get the data for the batch and output (target)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "       # Transfer data from CPU to GPU\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # Don't accumulate gradients during forward propagation. Set gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "       # Do forward pass of the input data through the network\n",
        "        output = model(data)\n",
        "        # Calculate loss once forward pass is done\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # Compute loss by moving backward and calculate the gradients\n",
        "        loss.backward()\n",
        "        # Update the model parameters based on the gradients accumulated during backprop\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "g_vlC-bdNzo1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "d1f19a30-565f-4557-fc1f-215f760c3a3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-4-41ccc38eb45d>:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.08230243623256683 batch_id=468: 100%|██████████| 469/469 [00:28<00:00, 16.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0622, Accuracy: 9790/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6agTEkqzz6TZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}